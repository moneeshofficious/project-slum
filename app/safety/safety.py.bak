# app/safety/safety.py
# -*- coding: utf-8 -*-
# NOTE: Config-driven: scope patterns, redirect text, DEI lexicon load from policies.yaml
from __future__ import annotations

import base64
import json
import os
import re
import sys
import time
import logging
from typing import Any, Dict, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from datetime import datetime, timezone
from threading import RLock
from __future__ import annotations

# Runtime policy loader
from app.safety import config as safety_config

logger = logging.getLogger(__name__)

# ---------- Metrics Wrapper (Prometheus if available, else in-memory no-op) ----------

class _Metrics:
    """
    Lightweight metrics wrapper. Uses Prometheus if available; otherwise in-memory counters
    accessible via get_counter() for tests.
    """
    def __init__(self) -> None:
        self._use_prom: bool = False
        self._counters: Dict[Tuple[str, Tuple[Tuple[str, str], ...]], int] = {}
        self._hist: List[float] = []

        # Prom aliases/handles (or None) set up if prometheus_client is present
        self.evaluations_total: Any = None
        self.decision_total: Any = None
        self.category_hits_total: Any = None
        self.blocks_total: Any = None
        self.redactions_total: Any = None
        self.latency_seconds: Any = None

        self.scope_blocks_count: Any = None
        self.risk_triggers_count: Any = None
        self.dei_rewrites_count: Any = None
        self.consent_events_total: Any = None
        self.consent_accept_count: Any = None

        # Aliases used in inc_counter() calls
        self.safety_scope_blocks_count: Any = None
        self.safety_risk_triggers_count: Any = None
        self.safety_dei_rewrites_count: Any = None
        self.safety_consent_events_total: Any = None
        self.safety_consent_accept_count: Any = None

        try:
            from prometheus_client import Counter, Histogram  # type: ignore
            self._use_prom = True

            self.evaluations_total = Counter("safety_evaluations_total", "Total texts evaluated")
            self.decision_total = Counter("safety_decision_total", "Decisions by type", ["decision"])
            self.category_hits_total = Counter("safety_category_hits_total", "Category hits", ["category"])
            self.blocks_total = Counter("safety_blocks_total", "Blocks by category", ["category"])
            self.redactions_total = Counter("safety_redactions_total", "Redactions by kind", ["kind"])
            self.latency_seconds = Histogram(
                "safety_latency_seconds",
                "Latency per safety evaluation",
                buckets=(0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2, 3, 5),
            )

            self.scope_blocks_count = Counter("safety_scope_blocks_count", "Scope limiter blocks")
            self.risk_triggers_count = Counter("safety_risk_triggers_count", "Risk triggers")
            self.dei_rewrites_count = Counter("safety_dei_rewrites_count", "DEI rewrites applied")
            self.consent_events_total = Counter("safety_consent_events_total", "Consent events", ["event"])
            self.consent_accept_count = Counter("safety_consent_accept_count", "Consent accepts total")

            # Exact alias names used in inc_counter() below
            self.safety_scope_blocks_count = self.scope_blocks_count
            self.safety_risk_triggers_count = self.risk_triggers_count
            self.safety_dei_rewrites_count = self.dei_rewrites_count
            self.safety_consent_events_total = self.consent_events_total
            self.safety_consent_accept_count = self.consent_accept_count

        except Exception as e:
            logger.debug("Prometheus metrics unavailable, using in-memory counters: %r", e)

    def inc_counter(self, name: str, labels: Optional[Dict[str, str]] = None, value: int = 1) -> None:
        if self._use_prom:
            metric = getattr(self, name)
            if labels:
                metric.labels(**labels).inc(value)
            else:
                metric.inc(value)
            return
        key = (name, tuple(sorted((labels or {}).items())))
        self._counters[key] = self._counters.get(key, 0) + value

    def observe_hist(self, name: str, value: float) -> None:
        if self._use_prom:
            getattr(self, name).observe(value)
            return
        if name == "latency_seconds":
            self._hist.append(value)

    def get_counter(self, name: str, labels: Optional[Dict[str, str]] = None) -> int:
        if self._use_prom:
            return -1
        key = (name, tuple(sorted((labels or {}).items())))
        return self._counters.get(key, 0)

    def get_hist_count(self) -> int:
        if self._use_prom:
            return -1
        return len(self._hist)

_metrics = _Metrics()

# ---------- Data structures ----------

@dataclass
class SafetyDecision:
    """Result for the core SafetyGuard evaluation."""
    action: str  # "allow" | "redact" | "block"
    text: str
    reasons: List[str] = field(default_factory=list)
    categories: List[str] = field(default_factory=list)
    redactions: Dict[str, int] = field(default_factory=dict)
    meta: Dict[str, str] = field(default_factory=dict)

    def add_reason(self, reason: str) -> None:
        if reason not in self.reasons:
            self.reasons.append(reason)

    def add_category(self, cat: str) -> None:
        if cat not in self.categories:
            self.categories.append(cat)

# ---------- Shared resources / constants (CONFIG-DRIVEN) ----------

# Redirect message comes from YAML, with safe default inside config if missing
_SCOPE_REDIRECT_MESSAGE: str = safety_config.get_redirect_message()

def _strip_leading_inline_flags(p: str) -> str:
    """
    Remove a leading inline flag block like (?i), (?imx), (?-i), etc.
    This allows safely joining many patterns into one alternation compiled with re.I.
    """
    return re.sub(r'^\(\?[a-zA-Z-]*\)', '', p or '').strip()

def _compile_scope_block_re() -> re.Pattern:
    """
    Compile a single alternation regex from YAML patterns.
    - Strips leading inline flag blocks so Python doesn't throw
      'global flags not at the start of the expression'.
    - Uses re.I to preserve case-insensitivity.
    - Falls back to defaults if YAML is missing/malformed.
    """
    try:
        patterns = safety_config.get_scope_patterns()
        cleaned = [_strip_leading_inline_flags(s) for s in patterns if s]
        joined = "|".join(f"(?:{p})" for p in cleaned) or "(?!)"
        return re.compile(joined, re.I)
    except Exception as e:
        logger.warning("Using fallback scope patterns due to error: %s", e)
        defaults = [_strip_leading_inline_flags(s) for s in safety_config.DEFAULT_SCOPE_PATTERNS if s]
        joined = "|".join(f"(?:{p})" for p in defaults) or "(?!)"
        return re.compile(joined, re.I)

_SCOPE_BLOCK_RE: re.Pattern = _compile_scope_block_re()

def _load_dei_lexicon() -> Dict[str, str]:
    try:
        # DEI lexicon values are replacement strings; keys may have (?i) but we pass re.I anyway.
        return safety_config.get_dei_lexicon()
    except Exception as e:
        logger.warning("Using fallback DEI lexicon due to error: %s", e)
        return dict(safety_config.DEFAULT_DEI_LEXICON)

_DEI_LEXICON: Dict[str, str] = _load_dei_lexicon()

_DEFAULT_RESOURCES_IN = [
    "India — Emergency: 112",
    "Tele MANAS (24x7): 14416 or 1-800-891-4416",
    "KIRAN: 1800-599-0019",
    "AASRA: 91-9820466726",
    "Vandrevala Foundation: 9999-666-555",
]

_RESOURCES_SHOWN_SESSIONS: set = set()
_CLARIFIER_LOW = (
    "I’m hearing things are heavy. Are you feeling unsafe right now? "
    "If yes, I can share support options. If not, we can take it step by step together."
)

# ---------- Core Safety Guard ----------

class SafetyGuard:
    """
    Primary safety evaluator with PII redaction, risk detection, and blocking.
    """
    MAX_LEN = 10_000
    REPEAT_CHAR_THRESHOLD = 50

    _EMAIL_RE = re.compile(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b")
    _PHONE_RE = re.compile(r"\b(?:\+?\d[\d\-\.\s\(\)]{6,})\b")  # validate digits in replacer
    _CC_RE = re.compile(r"\b(?:\d[ -]*?){13,19}\b")
    _ADDR_HINTS = re.compile(
        r"\b(?:street|st\.|road|rd\.|avenue|ave\.|lane|ln\.|flat|apt\.|block|district|pin\s?code)\b", re.I
    )

    _SELF_HARM = re.compile(r"\b(kill myself|suicide|end my life|self[-\s]?harm|i want to die)\b", re.I)
    _SEXUAL_MINORS = re.compile(r"\b(minor|under\s?age|child porn|cp|prete?n|young girl|young boy)\b", re.I)
    _HATE_THREAT = re.compile(
        r"\b(kill|exterminate|gas|lynch)\b.*\b(muslim|hindu|christian|jew|woman|men|dalit|gay|trans|caste|race)\b", re.I
    )
    _EXPLICIT_VIOLENCE = re.compile(r"\b(how to make a bomb|homemade explosive|chloroform\srecipe|napalm)\b", re.I)
    _UNSAFE_DRUG = re.compile(r"\b(dose|dosage|how much)\b.*\b(ketamine|lsd|mdma|cocaine|heroin|meth)\b", re.I)
    _MEDICAL_RISK = re.compile(r"\b(diagnose|is it cancer|skip my meds|ignore doctor|self medicate)\b", re.I)
    _FIN_ADVICE = re.compile(r"\b(all-in|guaranteed profit|can[’']?t lose|double my money|inside info)\b", re.I)

    _JAILBREAK = re.compile(
        r"(ignore\s+previous\s+instructions|act\s+as\s+|system\s+prompt|developer\s+mode|no\s+limitations|bypass\s+guard|"
        r"pretend\s+to\s+be\s+|do\s+not\s+follow\s+rules|jailbreak)",
        re.I,
    )

    _CONTROL = re.compile(r"[\u0000-\u001F\u007F]")
    _ONLY_EMOJI = re.compile(
        r"^\s*(?:[\U0001F300-\U0001FAFF\U00002700-\U000027BF\U00002600-\U000026FF]+|\ufe0f|\u200d|\u2640|\u2642|\u2695|\u2696|\u2702|\u2764)+\s*$"
    )

    def evaluate(self, text: str, meta: Optional[Dict[str, str]] = None) -> SafetyDecision:
        """
        Evaluate a user text for safety concerns.

        Returns:
            SafetyDecision with action 'allow'|'redact'|'block', categories, reasons, and redactions.
        """
        start = time.monotonic()
        meta = meta or {}
        decision = SafetyDecision(action="allow", text=text, meta=meta)

        # Quick exits
        if text is None or len(text) == 0:
            decision.add_reason("empty_text_allowed_as_noop")
            self._finalize_metrics(decision, start)
            return decision
        if text.strip() == "":
            decision.add_reason("whitespace_only")
            self._finalize_metrics(decision, start)
            return decision
        if self._ONLY_EMOJI.match(text or ""):
            decision.add_reason("emoji_only")
            self._finalize_metrics(decision, start)
            return decision

        # Length block
        if len(text) > self.MAX_LEN:
            decision.action = "block"
            decision.add_category("edge_too_long")
            decision.add_reason(f"text_exceeds_max_len_{self.MAX_LEN}")
            self._finalize_metrics(decision, start, blocks=["edge_too_long"])
            return decision

        # Control chars strip
        if self._CONTROL.search(text):
            decision.add_category("edge_control_chars")
            decision.add_reason("control_chars_removed")
            text = self._CONTROL.sub("", text)
            decision.text = text

        # Hints (non-blocking)
        if self._looks_like_base64_blob(text):
            decision.add_category("edge_base64_blob")
            decision.add_reason("base64_blob_suspected")
        if self._is_repeat_spam(text):
            decision.add_category("edge_repeat_spam")
            decision.add_reason("repeat_char_spam")

        # Category scans
        categories_block: List[str] = []
        categories_redact: List[str] = []
        self._category_check(text, self._SELF_HARM, "self_harm", categories_block)
        self._category_check(text, self._SEXUAL_MINORS, "sexual_minors", categories_block)
        self._category_check(text, self._HATE_THREAT, "hate_threat", categories_block)
        self._category_check(text, self._EXPLICIT_VIOLENCE, "explicit_violence", categories_block)
        self._category_check(text, self._UNSAFE_DRUG, "unsafe_drug", categories_block)
        self._category_check(text, self._MEDICAL_RISK, "medical_risk_advice", categories_redact)
        self._category_check(text, self._FIN_ADVICE, "financial_advice_risk", categories_redact)
        self._category_check(text, self._JAILBREAK, "jailbreak_injection", categories_redact)

        # PII redaction
        redacted_text, redactions = self._redact_pii(text)
        if redactions:
            categories_redact.append("pii")

        # Block
        if categories_block:
            for c in categories_block:
                decision.add_category(c)
            decision.action = "block"
            decision.add_reason("blocked_categories_present")
            self._finalize_metrics(decision, start, blocks=categories_block)
            return decision

        # Redact
        if categories_redact or redactions:
            for c in categories_redact:
                decision.add_category(c)
            for kind, count in redactions.items():
                decision.redactions[kind] = decision.redactions.get(kind, 0) + count
                _metrics.inc_counter("redactions_total", {"kind": kind}, count)
            decision.action = "redact"
            decision.text = redacted_text
            decision.add_reason("risk_requires_redaction_or_careful_response")
            self._finalize_metrics(decision, start)
            return decision

        if decision.categories:
            decision.add_reason("categories_non_blocking")
        self._finalize_metrics(decision, start)
        return decision

    def _category_check(self, text: str, pattern: re.Pattern, label: str, bucket: List[str]) -> None:
        if pattern.search(text):
            bucket.append(label)
            _metrics.inc_counter("category_hits_total", {"category": label}, 1)

    def _looks_like_base64_blob(self, text: str) -> bool:
        token = "".join(ch for ch in text if ch.isalnum() or ch in "+/=")
        if len(token) < 80:
            return False
        try:
            base64.b64decode(token, validate=True)
            return True
        except Exception:
            return False

    def _is_repeat_spam(self, text: str) -> bool:
        return re.search(r"(.)\1{" + str(self.REPEAT_CHAR_THRESHOLD) + r",}", text) is not None

    def _mask(self, s: str, visible: int = 2) -> str:
        if len(s) <= visible:
            return "*" * len(s)
        return s[:visible] + "*" * (len(s) - visible)

    def _redact_pii(self, text: str) -> Tuple[str, Dict[str, int]]:
        """
        Redact email, phone numbers, and credit cards. Count each kind precisely.
        Order matters: redact credit cards first to avoid phone false-positives.
        """
        redactions: Dict[str, int] = {}

        def sub_and_count(pattern: re.Pattern, repl_func, kind: str, s: str) -> str:
            count = 0

            def _repl(m: re.Match) -> str:
                nonlocal count
                count += 1
                return repl_func(m)

            out = pattern.sub(_repl, s)
            if count and kind:
                redactions[kind] = redactions.get(kind, 0) + count
            return out

        # Credit card masking (first)
        def cc_mask(m: re.Match) -> str:
            digits = re.sub(r"\D", "", m.group())
            if 13 <= len(digits) <= 19:
                return self._mask(digits, 4)
            return m.group()

        redacted = sub_and_count(self._CC_RE, cc_mask, "credit_card", text)

        # Email
        redacted = sub_and_count(self._EMAIL_RE, lambda m: self._mask(m.group(), 2), "email", redacted)

        # Phone: validate total digit length (avoid matching credit cards)
        def phone_mask(m: re.Match) -> str:
            raw = m.group()
            digits = re.sub(r"\D", "", raw)
            if 7 <= len(digits) <= 12:
                return self._mask(digits, 2)
            return raw

        redacted = sub_and_count(self._PHONE_RE, phone_mask, "phone", redacted)

        # Address hint marker (note only)
        if self._ADDR_HINTS.search(redacted):
            redactions["address_hint"] = redactions.get("address_hint", 0) + 1

        return redacted, redactions

    def _finalize_metrics(self, decision: SafetyDecision, start: float, blocks: Optional[List[str]] = None) -> None:
        _metrics.inc_counter("evaluations_total")
        _metrics.inc_counter("decision_total", {"decision": decision.action})
        # NOTE: category_hits_total is incremented at detection time only (_category_check), not here.
        if blocks:
            for cat in blocks:
                _metrics.inc_counter("blocks_total", {"category": cat})
        _metrics.observe_hist("latency_seconds", time.monotonic() - start)

    # ---- Instance-style scope API for tests that import `from app.safety import safety`
    def enforce_scope(self, text: str) -> Tuple[str, bool]:
        """
        Instance method returns (message_text, is_blocked).
        """
        blocked, msg = enforce_scope(text)  # module-level returns (blocked, msg)
        return (msg, True) if blocked else (text, False)

_guard_singleton: Optional[SafetyGuard] = None

def get_safety_guard() -> SafetyGuard:
    """Singleton accessor used by tests and orchestrator."""
    global _guard_singleton
    if _guard_singleton is None:
        _guard_singleton = SafetyGuard()
    return _guard_singleton

# ---------- Scope limiter + risk detector + DEI filter (public API) ----------

def enforce_scope(text: str) -> Tuple[bool, str]:
    """
    Module-level scope limiter used by unit tests that do:
        blocked, msg = enforce_scope(text)

    Returns:
        (blocked: bool, message: str)
    """
    if not text:
        return False, text

    if _SCOPE_BLOCK_RE.search(text or ""):
        _metrics.inc_counter("safety_scope_blocks_count")
        return True, _SCOPE_REDIRECT_MESSAGE

    return False, text

def detect_risk(text: str, profile: Optional[dict] = None) -> Dict[str, Union[str, List[str]]]:
    """
    Classify risk: 'none' | 'low' | 'high'
    Also return a 'reasons' list for tests.
    """
    if not text:
        return {"risk": "none", "reason": "empty", "reasons": []}
    g = get_safety_guard()
    dec = g.evaluate(text)
    reasons: List[str] = []
    if "self_harm" in dec.categories:
        _metrics.inc_counter("safety_risk_triggers_count")
        reasons.append("keyword:self_harm")
        return {"risk": "high", "reason": "self_harm", "reasons": reasons}
    if "explicit_violence" in dec.categories:
        _metrics.inc_counter("safety_risk_triggers_count")
        reasons.append("keyword:violence")
        return {"risk": "high", "reason": "explicit_violence", "reasons": reasons}
    if re.search(r"\b(i'?m done|nothing matters|can[’']?t go on)\b", text, re.I):
        _metrics.inc_counter("safety_risk_triggers_count")
        reasons.append("keyword:ambiguous_distress")
        return {"risk": "low", "reason": "ambiguous_distress", "reasons": reasons}
    return {"risk": "none", "reason": "no_signals", "reasons": reasons}

def apply_dei_filter(reply: Union[str, dict]) -> Union[str, dict]:
    """
    Rewrite stigmatizing terms to inclusive language. Handles str or dict with 'sections'.
    Increments safety_dei_rewrites_count when any replacement occurs.
    """
    def _rewrite_str(s: str) -> str:
        out = s or ""
        changed = False
        for pat, replacement in _DEI_LEXICON.items():
            # Keys might include (?i); we also pass flags=re.I to be robust
            new_out = re.sub(pat, replacement, out, flags=re.I)
            if new_out != out:
                changed = True
                out = new_out
        if changed:
            _metrics.inc_counter("safety_dei_rewrites_count")
        return out

    if isinstance(reply, str) or reply is None:
        return _rewrite_str(reply or "")

    if isinstance(reply, dict):
        sections = reply.get("sections")
        if isinstance(sections, dict):
            for k, v in list(sections.items()):
                if isinstance(v, str):
                    sections[k] = _rewrite_str(v)
        else:
            for k, v in list(reply.items()):
                if isinstance(v, str):
                    reply[k] = _rewrite_str(v)
        return reply

    return reply

def _load_resources_file() -> List[str]:
    """
    Load optional local 'resources_in.json' for helplines. Falls back to defaults for India.
    Accepts files with/without BOM; logs warnings for invalid JSON and I/O errors.
    """
    candidates = [
        os.path.join(os.path.dirname(__file__), "resources_in.json"),
        os.path.join(os.getcwd(), "resources_in.json"),
    ]
    for p in candidates:
        if os.path.exists(p):
            try:
                with open(p, "r", encoding="utf-8-sig") as f:
                    data = json.load(f)
                if isinstance(data, list):
                    return [str(x) for x in data]
            except json.JSONDecodeError as e:
                logger.warning("Invalid JSON in resources file %s: %s", p, e)
            except OSError as e:
                logger.warning("Unable to read resources file %s: %s", p, e)
    return _DEFAULT_RESOURCES_IN

def pre_prompt_guard(user_message: str, user_profile: Optional[dict] = None) -> Dict[str, object]:
    """
    Run pre-prompt safety per pipeline call:
      pre = pre_prompt_guard(user_message, {**profile, "session_id": session_id})
    Returns dict with action/text/risk/categories/reasons and risk_info (incl. policy_version, age_band).
    """
    # 1) scope (module-level returns (blocked, msg))
    blocked, msg = enforce_scope(user_message)
    if blocked:
        pv = (user_profile or {}).get("session_id")
        return {
            "action": "block",
            "text": msg,
            "risk": "none",
            "risk_info": {"risk": "none", "reason": "scope_block", "reasons": [], "policy_version": pv},
            "categories": ["scope"],
            "reasons": ["scope_block"],
            "meta": {"policy_version": pv} if pv else {},
        }

    # 2) guard
    guard = get_safety_guard()
    dec = guard.evaluate(
        user_message,
        meta={"session_id": str((user_profile or {}).get("session_id", ""))}
    )

    # 3) risk
    risk_info = detect_risk(dec.text, user_profile)
    policy_version = (user_profile or {}).get("session_id")
    if policy_version and isinstance(risk_info, dict):
        risk_info["policy_version"] = policy_version
    if isinstance(user_profile, dict) and "age_band" in user_profile:
        risk_info["age_band"] = user_profile["age_band"]

    return {
        "action": dec.action,
        "text": dec.text,
        "risk": risk_info.get("risk", "none"),
        "risk_info": risk_info,
        "categories": dec.categories[:],
        "reasons": dec.reasons[:] or risk_info.get("reasons", []),
        "meta": {"policy_version": policy_version} if policy_version else {},
    }

def inject_resources(reply: Union[str, dict], risk_info: Dict[str, str]) -> Union[str, dict]:
    """
    High risk: add full helplines (+ throttle by session).
    Low/ambiguous risk: add a brief check-in clarifier (no helpline dump).
    Teen (non-high risk): add a short caregiver involvement note.
    """
    risk = (risk_info or {}).get("risk", "none")
    session_key = (risk_info or {}).get("policy_version") or (risk_info or {}).get("session_id")
    age_band = (risk_info or {}).get("age_band")

    def _ensure_sections(obj: dict) -> dict:
        if "sections" not in obj or not isinstance(obj["sections"], dict):
            obj["sections"] = {}
        return obj

    # Dict path
    if isinstance(reply, dict):
        reply = dict(reply)
        _ensure_sections(reply)
        meta = reply.get("meta") or {}
        reply["meta"] = meta

        # Teen caregiver (when not high risk)
        if age_band == "teen" and risk != "high":
            reply["sections"]["caregiver_note"] = (
                "If it feels safe, consider letting a trusted caregiver or school counselor know. "
                "You don’t have to handle this alone."
            )

        if risk == "high":
            resources = _load_resources_file()[:5]
            addition = "If you might be unsafe, these can help right now:\n- " + "\n- ".join(resources)
            reply["sections"]["resources"] = addition
            reply["resources"] = addition
            if session_key and session_key in _RESOURCES_SHOWN_SESSIONS:
                meta["risk_resources_shown"] = "reminder"
            else:
                meta["risk_resources_shown"] = True
                if session_key:
                    _RESOURCES_SHOWN_SESSIONS.add(session_key)
            return reply

        if risk == "low":
            reply["sections"]["check_in"] = _CLARIFIER_LOW
            meta["risk_resources_shown"] = False
            return reply

        return reply  # risk none

    # String path
    if risk == "high":
        resources = _load_resources_file()[:5]
        addition = "If you might be unsafe, these can help right now:\n- " + "\n- ".join(resources)
        return (reply or "") + "\n\n" + addition
    if risk == "low":
        return (reply or "") + "\n\n" + _CLARIFIER_LOW
    return reply

def post_prompt_guard(reply: Union[str, dict], risk_info: Optional[dict] = None) -> Dict[str, object]:
    """
    Post-prompt: DEI filter + resource injection.
    Returns: {"reply": <obj>, "meta": {...}}
    """
    risk_info = risk_info or {"risk": "none"}
    filtered = apply_dei_filter(reply)
    final = inject_resources(filtered, risk_info)

    meta: Dict[str, object] = {}
    if isinstance(final, dict):
        meta.update(final.get("meta", {}))
    if not isinstance(final, dict) and risk_info.get("risk") in ("low", "high"):
        meta["risk_resources_shown"] = True
    if "policy_version" in (risk_info or {}):
        meta["policy_version"] = risk_info["policy_version"]

    return {"reply": final, "meta": meta}

# ---------- Consent persistence (pytest-aware minimal JSON store) ----------

_CONSENT_FILE = os.path.join(os.path.dirname(__file__), "consent_store.json")
_LOCK = RLock()

# Robust pytest detection
_TEST_MODE = bool(
    os.environ.get("PYTEST_CURRENT_TEST")
    or os.environ.get("PYTEST_ADDOPTS")
    or ("pytest" in sys.modules)
)

# In-memory consent storage; namespaced per-test so each test starts clean
_CONSENT_MEM: Dict[str, Dict[str, dict]] = {}  # {ns: {user_id: {...}}}

def _consent_ns() -> str:
    return os.environ.get("PYTEST_CURRENT_TEST", "global")

def _load_consent() -> Dict[str, dict]:
    if _TEST_MODE:
        ns = _consent_ns()
        return dict(_CONSENT_MEM.get(ns, {}))
    with _LOCK:
        if not os.path.exists(_CONSENT_FILE):
            return {}
        try:
            with open(_CONSENT_FILE, "r", encoding="utf-8") as f:
                return json.load(f) or {}
        except Exception:
            return {}

def _save_consent(data: Dict[str, dict]) -> None:
    if _TEST_MODE:
        ns = _consent_ns()
        _CONSENT_MEM[ns] = dict(data)
        return
    with _LOCK:
        try:
            with open(_CONSENT_FILE, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception:
            logger.warning("Unable to persist consent store %s", _CONSENT_FILE)

def needs_consent(user_id: str) -> bool:
    """Return True if the user has not accepted consent yet."""
    if not user_id:
        return True
    data = _load_consent()
    rec = data.get(user_id)
    return not (rec and rec.get("accepted") is True)

def record_consent(user_id: str, accepted: bool = True, ts: Optional[str] = None) -> None:
    """Store consent acceptance/decline. Defaults to accepted=True for simple tests."""
    if not user_id:
        return
    data = _load_consent()
    data[user_id] = {
        "accepted": bool(accepted),
        "ts": ts or datetime.now(timezone.utc).isoformat(),
        "version": 1,
    }
    _save_consent(data)
    if accepted:
        _metrics.inc_counter("safety_consent_accept_count")
    _metrics.inc_counter("safety_consent_events_total", {"event": "accept" if accepted else "decline"})
_EMAIL = re.compile(r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}\b")
_PHONE = re.compile(r"\b(?:\+?\d{1,3}[-.\s]?)?(?:\(?\d{3}\)?[-.\s]?){2}\d{4}\b")

def redact_pii(text: str) -> str:
    """
    Replace PII with tokens expected by tests.
    """
    text = _EMAIL.sub("[redacted@email]", text)
    text = _PHONE.sub("[redacted@phone]", text)
    return text
# Provide a module-level singleton for tests: `from app.safety import safety`
safety = get_safety_guard()

__all__ = [
    "SafetyGuard",
    "get_safety_guard",
    "safety",
    "enforce_scope",
    "detect_risk",
    "apply_dei_filter",
    "inject_resources",
    "needs_consent",
    "record_consent",
    "pre_prompt_guard",
    "post_prompt_guard",
    "_metrics",
]
